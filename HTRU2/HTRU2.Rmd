---
title: "High Time Resolution Universe (HTRU)"
author: "Jorge Sauceda"
date: "2022-08-10"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(R.utils)) install.packages("R.utils", repos = "http://cran.us.r-project.org")
if(!require(tibble)) install.packages("tibble", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

library(caret, warn = FALSE)
library(plotly, warn = FALSE)
library(R.utils, warn = FALSE)
library(tibble, warn = FALSE)
library(tidyverse, warn = FALSE)

dl <- tempfile()
df_link <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip"
download.file(df_link, dl)
dat <- read_csv(dl, col_names = FALSE)
colnames(dat) <- c("IP_mean", "IP_std_dev", "IP_excess_kurtosis", "IP_skewness",
                  "DM_SNR_mean", "DM_SNR_std_dev", "DM_SNR_excess_kurtosis",
                  "DM_SNR_skewness", "class")
dat$class <- as.factor(dat$class)

# Test set will be 20% of data set
set.seed(10, sample.kind = "Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = dat$class, times = 1, p = 0.2, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

# remove data structures that will not be needed hereafter
rm(dl, df_link, test_index)

options(digits = 3)
models <- c('GLM', 'KNN', 'RF', 'SVM', 'RDA')
```

## Data description

The High Time Resolution Universe (HTRU 2) data set was [originally published](<https://archive-beta.ics.uci.edu/ml/datasets/htru2>) by Robert Lyon using the CC BY 4.0 license. This data set reports **pulsar** candidates collected during the HTRU survey. Pulsars are a type of star, of considerable scientific interest. Candidates must be classified in to pulsar and non-pulsar classes to aid discovery.

More information about the HTRU survey can be found [here](https://www.mpifr-bonn.mpg.de/research/fundamental/htru). To learn more about other pulsar surveys, have a look at [this article](https://www.mpifr-bonn.mpg.de/research/fundamental/pulsarsurveys). You may also read the [original publication](https://doi.org/10.1093/mnras/stw656).

Each record is described by 8 continuous variables, and a single class variable. The first four are simple statistics obtained from the integrated pulse profile. This is an array of continuous variables that describe a wavelength-resolved version of the signal that has been averaged in both time and frequency. The remaining four variables are similarly obtained from the DM-SNR curve. These are summarized below:

1. Mean of the integrated profile.
2. Standard deviation of the integrated profile.
3. Excess kurtosis of the integrated profile.
4. Skewness of the integrated profile.
5. Mean of the DM-SNR curve.
6. Standard deviation of the DM-SNR curve.
7. Excess kurtosis of the DM-SNR curve.
8. Skewness of the DM-SNR curve.
9. Class

HTRU 2 Summary:

* 17,898 total examples
* 1,639 positive examples
* 16,259 negative examples

```{r Data inspection}
tibble(head(dat, 10))
summary(dat)
```

## Model training

In the following paragraphs, we attempt to determine an appropriate model for predicting the outcome in out-of-sample data sets. The algorithms to be trained using Cross-Validation (CV) with the **caret** R-package include:

* Generalized Lineal Model (GLM)
* k-Nearest Neighbors (KNN)
* Random Forests (RF)
* Support Vector Machine (SVM)
* Regularized Discriminant Analysis (RDA)

Finally, an ensemble of the previous models will be crafted.
NOTE: Please be patient, it will probably take a while to train all the models.

```{r Model training, warning=FALSE}
# Generalized Linear Model
fit_glm <- train(class ~ ., method = "glm", data = train_set)
y_hat_glm <- predict(fit_glm, test_set)
acc_glm <- confusionMatrix(y_hat_glm, factor(test_set$class))$overall[["Accuracy"]]

# k-Nearest Neighbors
fit_knn <- train(class ~ ., 
                 method = "knn", 
                 data = train_set,
                 tuneGrid = data.frame(k = seq(3, 15, 2)))
y_hat_knn <- predict(fit_knn, test_set)
acc_knn <- confusionMatrix(y_hat_knn, factor(test_set$class))$overall[["Accuracy"]]

# Random Forest
set.seed(10, sample.kind = "default") # if using R 3.6 or later
fit_rf <- train(class ~ .,
             method = "rf", 
             data = train_set, 
             nodesize = 1, 
             tuneGrid = data.frame(mtry = seq(2, 10, 2)))

imp_rf <- varImp(fit_rf)
y_hat_rf <- predict(fit_rf, newdata = test_set)
acc_rf <- confusionMatrix(y_hat_rf, test_set$class)$overall[["Accuracy"]]

# Support Vector Machine
fit_svm <- train(class ~ ., method = "svmRadial", data = train_set)
y_hat_svm <- predict(fit_svm, test_set)
acc_svm <- confusionMatrix(y_hat_svm, factor(test_set$class))$overall[["Accuracy"]]

# Regularized Discriminant Analysis
fit_rda <- train(class ~ ., method = "rda", data = train_set)
y_hat_rda <- predict(fit_rda, test_set)
acc_rda <- confusionMatrix(y_hat_rda, factor(test_set$class))$overall[["Accuracy"]]
```

## Model Plots

Let us learn more about the data characteristics from the models trained

```{r Model plots, echo=FALSE}
# KNN trained model
fig_knn <- plot_ly(x = fit_knn$results$k, 
                   y = fit_knn$results$Accuracy,
                   type = "scatter",
                   mode = "marker",
                   error_y = list(array = fit_knn$results$AccuracySD))
fig_knn <- fig_knn %>% 
  layout(title = "KNN Model",
         xaxis = list(title = "Neighbors"),
         yaxis = list(title = "Accuracy"))
fig_knn

# Random Forest: number of predictors and performance
plot(fit_rf, 
     main = "Random Forest - number of predictors", 
     highlight = TRUE)

# Random Forest: variable importance
fig_rf <- plot_ly(x = imp_rf$importance$Overall,
                  y = colnames(train_set[1:8]),
                  type = "bar")
fig_rf <- fig_rf %>% 
  layout(title = "Random Forest - variable importance",
         xaxis = list(title = "Importance"),
         yaxis = list(title = "Predictor Variable"))
fig_rf
```

From the KNN model plot, we observe that the number of neighbors used to model the data do not impact much the accuracy of the model on the training set. A similar observation arises from the accuracy of the Random Forest model as a function of the number of predictors. However, the Random Forest variable importance plot indicates that a substantial portion of the outcome can be predicted by taking into account a fairly small number of variables, namely the excess kurtosis, skewness and mean values of the integrated profile prevail.

## Model comparison

Let us inspect the out of sample performance of our models to compare their outputs

```{r Out-of-sample model plots, echo=FALSE}
# Models' comparison
fig <- plot_ly(x = test_set$class, name = "actual", type = 'histogram')
fig <- fig %>% add_trace(x = y_hat_glm, name = "GLM", type = 'histogram')
fig <- fig %>% add_trace(x = y_hat_knn, name = "KNN", type = 'histogram')
fig <- fig %>% add_trace(x = y_hat_rf, name = "RF", type = 'histogram')
fig <- fig %>% add_trace(x = y_hat_svm, name = "SVM", type = 'histogram')
fig <- fig %>% add_trace(x = y_hat_rda, name = "RDA", type = 'histogram')
fig <- fig %>%
  layout(title = "Out of sample predictions",
         xaxis = list(title = "Signal class"),
         yaxis = list(title = "Count"),
         barmode = 'group')
fig
```

At first glance, the histogram seems to indicate very similar out-of-sample performances for each model trained. Let us compare their performance statistics:

```{r Model statistics}
models_compare <- resamples(list(
  GLM = fit_glm,
  KNN = fit_knn,
  RF = fit_rf,
  SVM = fit_svm,
  RDA = fit_rda
))

summary(models_compare)
```

A summary of the models provides further evidence that the models herein used perform similarly regardless of the classification of the different algorithms. In other words, given that our models (linear, prototype, random forest, cost sensitive and discriminant analysis) exhibit equivalent accuracy values, we may focus on time efficiency for classifying pulsar surveys data in the academia.

## Ensemble

Let us now build an ensemble with the trained models and inspect its performance:

```{r Model ensemble}
rows <- 1:length(test_set$class)
n <- length(models)
y_hat <- sapply(rows, function(rows){
  x <- ifelse(y_hat_glm[rows] == 1, 1, 0)
  x <- x + ifelse(y_hat_knn[rows] == 1, 1, 0)
  x <- x + ifelse(y_hat_rf[rows] == 1, 1, 0)
  x <- x + ifelse(y_hat_svm[rows] == 1, 1, 0)
  x <- x + ifelse(y_hat_rda[rows] == 1, 1, 0)
  ifelse(x >= 3, 1, 0)
}) %>% factor

confusionMatrix(y_hat, test_set$class)
```

## Conclusion

From our results, the shape of the integrated pulse profile and its statistics are the main factors to determine whether the spectra measured by the detector corresponds to a pulsar or background noise. Moreover, the choice of the classification algorithm does not impact much the accuracy of the results nor an ensemble improves the output much, and the choice of the classification algorithm can be focused on time efficiency in an academic setup.